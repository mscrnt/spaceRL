{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependancies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from gym.wrappers import Monitor\n",
    "from gym import logger as gymlogger\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "import gym\n",
    "from IPython import display as ipythondisplay\n",
    "import keras\n",
    "from keras.callbacks import TensorBoard\n",
    "import os.path\n",
    "import time\n",
    "import shutil\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from collections import deque \n",
    "from datetime import datetime\n",
    "from keras.models import clone_model\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_step_num = 10000 # The frequency at which the target network is updated\n",
    "batch_size = 32 # Number of training cases that are computed in each update\n",
    "gamma = 0.99 # Discount factor\n",
    "replay_memory = 350000 # This many recent frames are used for sampling\n",
    "num_episode = 100000 # The number of episodes\n",
    "learning_rate = 0.00025 # The learning rate\n",
    "init_epsilon = 1.0 # Initial value of epsilon in epsilon-greedy method\n",
    "final_epsilon = 0.1 # Final value of epsilon in epslion-greedy method\n",
    "epsilon_step_num = 1000000 # Number of frames to get from the initial to final value of epsilon\n",
    "refresh_target_model_num = 10000 # The frequency at which the target model updates\n",
    "no_op_steps = 30 # Max number of \"do nothing\" steps at the beginning of an episode\n",
    "train_dir = \"training_dir\" # Training directory\n",
    "tensorboard_dir = \"./logs\" # Tensorboard directory\n",
    "restore_file_path = \"./trained/SpaceInvaders_20221225073655.h5\" # Restore file path\n",
    "model_restore_dir = \"./model/SpaceInvaders_20221225073655.h5\" # Model file path\n",
    "num_test_episodes = 1 # Number of episodes you test for\n",
    "LOG_DIR = './logs'\n",
    "resume = False\n",
    "!mkdir logs # Make logs folder\n",
    "!mkdir training_dir # Make training_dir folder\n",
    "!mkdir model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84 , 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "  \n",
    "# Takes frame \"observe\", converts to grayscale, and crops to 84 by 84 square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y, q_value):\n",
    "    error = K.abs(y - q_value)\n",
    "    quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "    linear_part = error - quadratic_part\n",
    "    loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "    return loss\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATARI_SHAPE = (84, 84, 4) # 84 by 84 and 4-stack to help Neural Net determine direction\n",
    "ACTION_SIZE = 4\n",
    "\n",
    "def atari_model():\n",
    "    \n",
    "    # Input layers.\n",
    "    frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')\n",
    "    actions_input = keras.layers.Input((ACTION_SIZE,), name='mask')\n",
    "\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input) # Lambda is used in Keras to perform math\n",
    "    \n",
    "    # \"The first hidden layer convolves 32 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = keras.layers.Conv2D(\n",
    "        32, (8, 8), strides=4, activation='relu', name=\"Conv1\"\n",
    "    )(normalized)\n",
    "    \n",
    "    # \"The second hidden layer convolves 64 4×4 filters with stride 2, followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = keras.layers.Conv2D(\n",
    "        64, (4, 4), strides=2, activation='relu', name=\"Conv2\"\n",
    "    )(conv_1)\n",
    "    \n",
    "    # \"The third hidden layer convolves 64 3x3 filters with stride 1, again followed by a rectifier nonlinearity.\"\n",
    "    conv_3 = keras.layers.Conv2D(\n",
    "        64, (3, 3), strides=1, activation='relu', name=\"Conv3\"\n",
    "    )(conv_2)\n",
    "    \n",
    "    # Flattening the convolutional layer.\n",
    "    conv_flattened = keras.layers.core.Flatten(name=\"flatten\")(conv_3) # Taking an array and converting it into a linear vector\n",
    "    \n",
    "    # \"Fully connected layer made up of 512 rectifier units.\"\n",
    "    hidden = keras.layers.Dense(512, activation='relu', name=\"Dense512\")(conv_flattened)\n",
    "    \n",
    "    # The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = keras.layers.Dense(ACTION_SIZE, name=\"Output\")(hidden)\n",
    "    \n",
    "    # Finally, we multiply the output by the mask.\n",
    "    filtered_output = keras.layers.Multiply(name='QValue')([output, actions_input])\n",
    "\n",
    "    model = keras.models.Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "    model.compile(optimizer, loss=huber_loss)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up TensorBoard using ngrok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs'\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(tensorboard_dir)\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get action from model using epsilon-greedy policy\n",
    "def get_action(history, epsilon, step, model):\n",
    "    if np.random.rand() <= epsilon or step <= observe_step_num:\n",
    "        return random.randrange(ACTION_SIZE)\n",
    "    else:\n",
    "        q_value = model.predict([history, np.ones(ACTION_SIZE).reshape(1, ACTION_SIZE)])\n",
    "        return np.argmax(q_value[0])\n",
    "\n",
    "\n",
    "# save sample <s,a,r,s'> to the replay memory\n",
    "def store_memory(memory, history, action, reward, next_history, dead):\n",
    "    memory.append((history, action, reward, next_history, dead))\n",
    "\n",
    "# get one hot\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "  \n",
    "# train model by taking a random batch from memory\n",
    "def train_memory_batch(memory, model_target, model):\n",
    "    mini_batch = random.sample(memory, batch_size)\n",
    "    \n",
    "    history = np.zeros((batch_size, ATARI_SHAPE[0],\n",
    "                        ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    next_history = np.zeros((batch_size, ATARI_SHAPE[0],\n",
    "                             ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    target = np.zeros((batch_size,))\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    for idx, val in enumerate(mini_batch):\n",
    "        history[idx] = val[0]\n",
    "        next_history[idx] = val[3]\n",
    "        action.append(val[1])\n",
    "        reward.append(val[2])\n",
    "        dead.append(val[4])\n",
    "\n",
    "    actions_mask = np.ones((batch_size, ACTION_SIZE))\n",
    "    next_Q_values = model_target.predict([next_history, actions_mask])\n",
    "\n",
    "    # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    for i in range(batch_size):\n",
    "        if dead[i]:\n",
    "            target[i] = -1\n",
    "        else:\n",
    "            target[i] = reward[i] + gamma * np.amax(next_Q_values[i])\n",
    "\n",
    "    action_one_hot = get_one_hot(action, ACTION_SIZE)\n",
    "    target_one_hot = action_one_hot * target[:, None]\n",
    "\n",
    "    h = model.fit(\n",
    "        [history, action_one_hot], target_one_hot, epochs=1,\n",
    "        batch_size=batch_size, verbose=0)\n",
    "\n",
    "    return h.history['loss'][0]\n",
    "  \n",
    "  \n",
    "# Display Video\n",
    "  \n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "  env = Monitor(env, './video', force=True)\n",
    "  return env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Game\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    env = wrap_env(gym.make('SpaceInvaders-v4'))\n",
    "\n",
    "    episode_number = 0\n",
    "    epsilon = 0.001\n",
    "    global_step = 0\n",
    "    model = load_model(model_restore_dir, custom_objects={'huber_loss': huber_loss})  # load model with customized loss func\n",
    "\n",
    "    while episode_number < num_test_episodes:\n",
    "        \n",
    "        done = False\n",
    "        dead = False\n",
    "        \n",
    "        # 1 episode = 5 lives\n",
    "        score, start_life = 0, 3\n",
    "        observe = env.reset()\n",
    "\n",
    "        observe, _, _, _ = env.step(1)\n",
    "        # At start of episode, there is no preceding frame\n",
    "        # So just copy initial states to make history\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            # get action for the current history and go one step in environment\n",
    "            q_value = model.predict([history, np.ones(ACTION_SIZE).reshape(1, ACTION_SIZE)])\n",
    "            action = np.argmax(q_value[0])\n",
    "\n",
    "            # step in environment\n",
    "            observe, reward, done, info = env.step(action)\n",
    "            \n",
    "            # pre-process the observation --> history\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            # if the agent missed ball, agent is dead --> episode is not over\n",
    "            if isinstance(info, dict):\n",
    "                if start_life > info['ale.lives']:\n",
    "                    dead = True\n",
    "                    start_life = info['ale.lives']\n",
    "            elif isinstance(info, bool):\n",
    "                dead = info\n",
    "\n",
    "            # TODO: may be we should give negative reward if miss ball (dead)\n",
    "            score += reward\n",
    "            \n",
    "            # If agent is dead, set the flag back to false, but keep the history unchanged,\n",
    "            # to avoid to see the ball up in the sky\n",
    "            if dead:\n",
    "              dead = False\n",
    "              print (\"agent is dead\")\n",
    "              history = next_history\n",
    "            else:\n",
    "              history = next_history\n",
    "              \n",
    "            # print(\"step: \", global_step)\n",
    "            global_step += 1\n",
    "\n",
    "            if done:\n",
    "                episode_number += 1\n",
    "                print('episode: {}, score: {}'.format(episode_number, score))\n",
    "                \n",
    "    env.close()\n",
    "    show_video()\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaceRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa9e27a46c245bc55e943d16719b3185461e8b57e0ce1d9dee9354577fcebe2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
